[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zoomalong",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Journal/2019-03-04-StorageAccountEncryption.html",
    "href": "Journal/2019-03-04-StorageAccountEncryption.html",
    "title": "Storage Account Encryption",
    "section": "",
    "text": "By default storage accounts are encrypted, and Microsoft holds the keys. The encryption that is used is AES 256 bit, as it is one of the strongest ciphers currently available.\nThe one big issue with this is that Microsoft owns the encryption key and potentially has unrestricted access to the users data. They have included the facility for the user to specify an encryption key, so you can meet your individual company security or regulatory compliance needs. To use your own key, you will need. * A Key Vault * Needs to be in the same region as the storage * Does not need to be in the same subscription * Storage needs permissions to access your Key Vault. * Need to grant wrapKey, unwrapKey privileges"
  },
  {
    "objectID": "Journal/2019-03-04-StorageAccountEncryption.html#data-encryption",
    "href": "Journal/2019-03-04-StorageAccountEncryption.html#data-encryption",
    "title": "Storage Account Encryption",
    "section": "Data Encryption",
    "text": "Data Encryption\nA key point to understand, the data itself is not encrypted with the key. Microsoft employs a two stage encryption process which involves a DEK (Data Encryption Key) and a KEK (Key Encryption Key). The DEK is generated when the storage account is created, and is used to encrypt the data. The DEK is it self is encrypted with a key that Microsoft holds (KEK), Its this Microsoft key that can be replaced with the users own key."
  },
  {
    "objectID": "Journal/2019-03-04-StorageAccountEncryption.html#key-rotation",
    "href": "Journal/2019-03-04-StorageAccountEncryption.html#key-rotation",
    "title": "Storage Account Encryption",
    "section": "Key Rotation",
    "text": "Key Rotation\nThis is currently under development by Microsoft. Key rotation is a process where the KEK (see Data Encryption above) is rotated every 90 days, this involves decrypting the existing key and re-encrypting it with a newly generated key."
  },
  {
    "objectID": "Journal/2019-03-04-StorageAccountEncryption.html#references",
    "href": "Journal/2019-03-04-StorageAccountEncryption.html#references",
    "title": "Storage Account Encryption",
    "section": "References",
    "text": "References\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-service-encryption"
  },
  {
    "objectID": "Journal/2019-02-12-PfxBase64.html",
    "href": "Journal/2019-02-12-PfxBase64.html",
    "title": "Certificate pfx to Base64",
    "section": "",
    "text": "To convert certificate that is in .pfx to base64 format in PowerShell, you can use .NET namespace available in PowerShell to convert. I had a scenario where I was required to use base64 encoding to upload certificate to Azure to secure communication to backend instance. Since Microsoft Azure provides rich API to work with. I was able to make a patch request and push certificate to Azure. In this tutorial, I will show you how to convert certificate from .pfx to base64. Open PowerShell as an administrator. Now that we have PowerShell console opened. Let’s first load the content into a variable.\nParam([string] $file) $file = '.\\certificate.pfx' $pfxFileBytes = get-content $file -Encoding Byte [System.Convert]::ToBase64String($pfxFileBytes) | Out-File 'PfxFileBytes-Base64.txt'\nOnce you have your Base64 string you can insert it in the ARM template as shown below."
  },
  {
    "objectID": "Journal/2019-02-12-PfxBase64.html#azuredeploy.parameters.json",
    "href": "Journal/2019-02-12-PfxBase64.html#azuredeploy.parameters.json",
    "title": "Certificate pfx to Base64",
    "section": "Azuredeploy.parameters.json",
    "text": "Azuredeploy.parameters.json\n    \"cert\": {\"value\": \"MIIT...\"},\n    \"certPass\": { \"value\": \"password\" }"
  },
  {
    "objectID": "Journal/2019-02-12-PfxBase64.html#azuredeploy.json",
    "href": "Journal/2019-02-12-PfxBase64.html#azuredeploy.json",
    "title": "Certificate pfx to Base64",
    "section": "Azuredeploy.json",
    "text": "Azuredeploy.json\n  \"parameters\": {\n      \"cert\": {\n      \"type\": \"securestring\"\n    },\n    \"certPass\": {\n      \"type\": \"securestring\"\n    }\n  }\n  \"variables\": {\n    \"var_cert_name\": \"[concat( tolower(parameters('para_application_name')), uniqueString(resourceGroup().id))]\",\n  }\n  \"resources\": [\n      {\n      \"apiVersion\": \"2015-08-01\",\n      \"location\": \"[resourceGroup().location]\",\n      \"name\": \"[variables('var_cert_name')]\",\n      \"properties\": {\n        \"pfxBlob\": \"[parameters('cert')]\",\n        \"password\": \"[parameters('certPass')]\"\n      },\n      \"scale\": null,\n      \"tags\": {\n        \"displayName\": \"Certificate\"\n      },\n      \"type\": \"Microsoft.Web/certificates\"\n    }\n  ]"
  },
  {
    "objectID": "Journal/2019-02-11-KeyVault.html",
    "href": "Journal/2019-02-11-KeyVault.html",
    "title": "KeyVault",
    "section": "",
    "text": "I’m always looking for ways to simply automate deployments, so I don’t have to hard code, or repeat myself. This blog is looking at Key Vault deployments, and its use."
  },
  {
    "objectID": "Journal/2019-02-11-KeyVault.html#parameters",
    "href": "Journal/2019-02-11-KeyVault.html#parameters",
    "title": "KeyVault",
    "section": "Parameters",
    "text": "Parameters\nFor Key Vaults I don’t like to specify the normal key/value pairs, as it would mean that I have to specify the name of the secret within the deployment script. The obvious draw back here is, as the code evolves and the secret changes then you would need to update both the parameters plus the deployment script. So first off in the parameters file put the secrets into an array.\n    \"para_kvSecretsObject\": {\n      \"value\": {\n        \"secrets\": [\n          {\n            \"secretName\": \"applicationuser\",\n            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n          },\n          {\n            \"secretName\": \"AnotherSecret\",\n            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n          }\n        ]\n      }\nStraight away you can see that the actual secret is injected from the CI tool, this is so you can inject a different secret depending on the environment. Putting the secrets into an array in this way means that we can cycle through the array in the deployment script without actually referring to the secret itself, only using the array index."
  },
  {
    "objectID": "Journal/2019-02-11-KeyVault.html#load-secrets",
    "href": "Journal/2019-02-11-KeyVault.html#load-secrets",
    "title": "KeyVault",
    "section": "Load Secrets",
    "text": "Load Secrets\nLoading of secrets is normally achieved via the actual KeyVault deployment specifying each secret in turn, but as our secrets are specified as part of an array, we will need to apply them in a slightly different way. At the resource level we create a new feature which has a type of Microsoft.KeyVault/vaults/secrets. In the following code block the lines to look out for are the Copy/Name and Value properties. * The Copy property looks at the array and determines the number of items we need to process. * The Name property set the secret name to the ‘secrectName’ value in the array and it is indexed by the copyIndex() parameter * The Value is specified in the same way as the Name just using the secretValue array name.\nOne important line which is often overlooked is the DependsOn property. Each resource in the deployment file can be applied by Azure in parallel so by specifying the DependsOn property stops the deployment until the dependant resource exists.\n    {\n      \"apiVersion\": \"2015-06-01\",\n      \"copy\": {\n        \"name\": \"secretsCopy\",\n        \"count\": \"[length(parameters('para_kvSecretsObject').secrets)]\"\n      },\n      \"dependsOn\": [\n        \"[concat('Microsoft.KeyVault/vaults/', variables('var_kv_name'))]\"\n      ],\n      \"name\": \"[concat(variables('var_kv_name'), '/', parameters('para_kvSecretsObject').secrets[copyIndex()].secretName)]\",\n      \"properties\": {\n        \"value\": \"[parameters('para_kvSecretsObject').secrets[copyIndex()].secretValue]\"\n      },\n      \"tags\": {\n        \"displayName\": \"Key Vault Secrets\"\n      },\n      \"type\": \"Microsoft.KeyVault/vaults/secrets\"\n    }"
  },
  {
    "objectID": "Journal/2019-02-11-KeyVault.html#accessing-the-key-vault",
    "href": "Journal/2019-02-11-KeyVault.html#accessing-the-key-vault",
    "title": "KeyVault",
    "section": "Accessing the Key Vault",
    "text": "Accessing the Key Vault\nA Key Vault cannot exist on its own, when it is deployed it needs the service principal, at the moment the service principal can only be obtained from an AD associated application, such as a web app/api app or function.\n\n\n\nPlantUml flowchart"
  },
  {
    "objectID": "Journal/2019-02-08-PlantUml.html",
    "href": "Journal/2019-02-08-PlantUml.html",
    "title": "PlantUml",
    "section": "",
    "text": "PlantUML is not directly supported by GitHub, but its still possible. Basically we pass your puml file to PlantUML to generate, and they return a PNG which gets included in the page\n\nGenerate you .puml file in the usual way using your favorite editor and confirm the diagram is as you want it.\nSave the file to the assets directory\nThen paste the following into you Markup.\n\nPlantUML is a text descriptive language which gets converted into UML and other types of graphs So this :-\n@startuml\n\nBob-&gt;Alice : hello\n\n@enduml\nwould produce this :-\n\n\n\nPlantUml flowchart\n\n\nA more complex example :-\n![PlantUml flowchart](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.github.com/\nnewportg/newportg.github.io/assets/Plantuml/test.puml)\n\n\n\nPlantUml flowchart"
  },
  {
    "objectID": "Journal/2018-10-15-ARMTemplateParameter-VariableSetup.html",
    "href": "Journal/2018-10-15-ARMTemplateParameter-VariableSetup.html",
    "title": "ARM Template Parameter/Variable Setup",
    "section": "",
    "text": "For something so simple, arm templates can become complex things, so I prefer to try to set some ground rules before I go to deep. N.B this works for me, and may not suit everyone 😉"
  },
  {
    "objectID": "Journal/2018-10-15-ARMTemplateParameter-VariableSetup.html#parameters",
    "href": "Journal/2018-10-15-ARMTemplateParameter-VariableSetup.html#parameters",
    "title": "ARM Template Parameter/Variable Setup",
    "section": "Parameters",
    "text": "Parameters\nI prefer to inject any unique values via a VSTS/VSO or if your prefer Azure DevOps deployment process. In the first part of the file I spell out the acronyms which form part of the naming convention for the resources, you could use nested templates for this, but I feel they add unnecessary complications, as the nested template must be available via a URL. The second part involves parameters that are specific to this application, such as the tenant id, application name etc.\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"para_acronym_region\": { \"value\": \"we\" },\n        \"para_acronym_resgrp\": { \"value\": \"resgrp\" },\n        \"para_acronym_appsvc\": { \"value\": \"appsvc\" },\n        \"para_acronym_svcpln\": { \"value\": \"svcpln\" },\n        \"para_acronym_stract\": { \"value\": \"str\" },\n        \"para_acronym_kv\": { \"value\": \"kv\" },\n        \"para_acronym_azfunc\": { \"value\": \"fn\" },\n        \"para_acronym_appin\": { \"value\": \"appins\" },\n        \"para_acronym_webapp\": { \"value\": \"webapp\" },\n        \"para_ad_tenantid\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_application_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_vanity_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_target_env\": { \"value\": \"dev\" },\n        \"para_kvSecretsObject\": {\n            \"value\": {\n                \"secrets\": [\n                        {\n                        \"secretName\": \"applicationuser\",\n                        \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                        },\n                        {\n                            \"secretName\": \"AnotherSecrect\",\n                            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                        }\n                    ]\n                }\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "Journal/2018-10-15-ARMTemplateParameter-VariableSetup.html#variables",
    "href": "Journal/2018-10-15-ARMTemplateParameter-VariableSetup.html#variables",
    "title": "ARM Template Parameter/Variable Setup",
    "section": "Variables",
    "text": "Variables\nAs you can see from the variables, I build up my resource names from the parameters. I also pull in values for the hostingplan and component identities, so they can be used easily with the resource definitions.\n\"variables\": {\n    \"var_env_region\": \"[concat(parameters('para_target_env'), '-', parameters('para_acronym_region'))]\",\n    \"var_public_url\": \"[concat(parameters('para_target_env'), '.', parameters('para_application_name'), '.', parameters('para_vanity_name'))]\",\n    \"var_str_name\": \"[concat(parameters('para_application_name'), parameters('para_acronym_stract'), parameters('para_target_env'), parameters('para_acronym_region'))]\",\n    \"var_str_resId\": \"[resourceId(resourceGroup().Name,'Microsoft.Storage/storageAccounts', variables('var_str_name'))]\",\n    \"var_kv_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_kv'), '-', variables('var_env_region'))]\",\n    \"var_azf_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_azfunc'),'-', variables('var_env_region'))]\",\n    \"var_appin_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appin'),'-', variables('var_env_region'))]\",\n    \"var_hstpln_group\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_resgrp'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_env\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appsvc'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_svcpln'), '-', variables('var_env_region'))]\",\n    \"var_webapp_name\": \"[concat(parameters('para_application_name'), '-' ,parameters('para_acronym_webapp'),'-', variables('var_env_region'))]\",\n    \"var_webapp_hstpln\": \"[concat('/subscriptions/', subscription().subscriptionId, '/resourceGroups/', variables('var_hstpln_group'), '/providers/Microsoft.Web/serverfarms/', variables('var_hstpln_name'))]\",\n    \"var_msi_azf\": \"[concat(resourceId('Microsoft.Web/sites', variables('var_azf_name')),'/providers/Microsoft.ManagedIdentity/Identities/default')]\"\n},"
  },
  {
    "objectID": "Journal/2018-08-14-Hosting AzureStaticWebsites.html",
    "href": "Journal/2018-08-14-Hosting AzureStaticWebsites.html",
    "title": "Hosting Azure Static Websites",
    "section": "",
    "text": "Microsoft announced that you can now enable static websites on a storage account. This will generate a new URL for your site, and enable read access to any static html files within the blob storage. There’s a link to Microsoft’s preview announcement here. https://azure.microsoft.com/en-us/blog/azure-storage-static-web-hosting-public-preview/\nAt the moment I prefer the approach made by Anthony Chu in his blog post https://anthonychu.ca/post/azure-functions-static-file-server/\nHe hosts a index.html file within a Azure function Http Trigger request. Although this only returns a static file, it does allow you to create a on demand website. The Html file you serve up can should only contain links to CDN resources or to readable JS or other files within your storage account blob storage. By proxying the azure function then everything could be accessed via the same URL.\nusing System.IO;\nusing System.Linq;\nusing System.Net;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Threading.Tasks;\nusing Microsoft.Azure.WebJobs;\nusing Microsoft.Azure.WebJobs.Extensions.Http;\nusing Microsoft.Azure.WebJobs.Host;\nusing MimeTypes;\n\nnamespace Counterflip\n{\n    public static class WebSite\n    {\n        [FunctionName(\"WebSite\")]\n        public static async Task&lt;HttpResponseMessage&gt; Run([HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)]HttpRequestMessage req, TraceWriter log)\n        {\n            log.Info(\"C# HTTP trigger function processed a request.\");\n\n            try\n            {\n                var response = new HttpResponseMessage(HttpStatusCode.OK);\n                var stream = new FileStream(@\"www\\index.html\", FileMode.Open);\n                response.Content = new StreamContent(stream);\n                response.Content.Headers.ContentType =\n                    new MediaTypeHeaderValue(GetMimeType(@\"www\\index.html\"));\n                return response;\n            }\n            catch\n            {\n                return new HttpResponseMessage(HttpStatusCode.NotFound);\n            }\n        }\n\n        private static string GetMimeType(string filePath)\n        {\n            var fileInfo = new FileInfo(filePath);\n            return MimeTypeMap.GetMimeType(fileInfo.Extension);\n        }\n    }\n}"
  },
  {
    "objectID": "Journal/2018-08-14-AzureApplicationHosting.html",
    "href": "Journal/2018-08-14-AzureApplicationHosting.html",
    "title": "Azure Application Hosting",
    "section": "",
    "text": "Each component is a separately deploy-able artefact, but we need a coherent single URL to link them all. The normal method would be to deploy out each individual component to Azure and each would get its own ‘aurewebsites.com’ URL. This approach would lead to confusion, as it would mean you would need to keep lists of URL’s By using the proxy feature of Azure Functions we can define routes to each of the installed artefacts while preserving a single URL for the application. So:-\n\n\n\nRoute\nResult\n\n\n\n\nzoomalong.co.uk\nWebsite\n\n\nzoomalong.co.uk/api\nAzure functions\n\n\nzoomalong.co.uk/static\nAzure Storage Account\n\n\n\nUsed to store any files or images etc. Because both the API and Web Application exist on the same URL then we won’t run into any CORS issues. Remember to bind the DNS Cname to the Azure function proxy and not the website.\n\n\n\naah1\n\n\nHow ? In your Azure Functions Project Create a files called proxies.json and insert the following code\nproxies.json\n{\n  \"$schema\": \"http://json.schemastore.org/proxies\",\n    \"proxies\": {\n      \"api\": {\n        \"matchCondition\": {\n          \"route\": \"/api/{*url}\"\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      },\n      \"app\": {\n        \"matchCondition\": {\n          \"route\": \"{*url}\",\n          \"methods\": [ \"GET\", \"HEAD\", \"OPTIONS\" ]\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      },\n      \"appResources\": {\n        \"matchCondition\": {\n          \"route\": \"/static/{*url}\",\n          \"methods\": [ \"GET\", \"HEAD\", \"OPTIONS\" ]\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      }\n    }\n  }\nChange in the build. * Build should include Azure App Service Deploy V3 or greater * Update this section with the file to be changed. *  * Add your substitutions to the Variables section * As Path to variable to be replace element.element.element * Value to be replaced. * as below\n\n\n\n\n\n\n\nProxy\nURL\n\n\n\n\nproxies.api.backendUri\nhttps://azure website url\n\n\nproxies.app.backendUri\nhttps://azure function url\n\n\nproxies.appResources.backendUri\nhttps:// azure storage account blob strorage\n\n\n\n\nRepeat in the release."
  },
  {
    "objectID": "Journal/2018-10-10-Arm-Template-ParameterVariable-Setup.html",
    "href": "Journal/2018-10-10-Arm-Template-ParameterVariable-Setup.html",
    "title": "ARM TEMPLATE PARAMETER/VARIABLE SETUP",
    "section": "",
    "text": "For something so simple, arm templates can become complex things, so I prefer to try to set some ground rules before I go to deep.\nN.B this works for me, and may not suit everyone 😉\nYou should employee a naming convention for your artifacts. Every Resource should be tagged. There should be a clear naming convention between the parameters and variables. Parameters should be either primitives or unique values Variables should build up your resource names from the parameter primitives. ### Parameters I prefer to inject any unique values via a VSTS/VSO or if your prefer Azure DevOps deployment process.\nIn the first part of the file I spell out the acronyms which form part of the naming convention for the resources, you could use nested templates for this, but I feel the add unnecessary complications, as the nested template must be available via a URL.\nThe second part involves parameters that are specific to this application, such as the tenant id, application name etc.\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"para_acronym_region\": { \"value\": \"we\" },\n        \"para_acronym_resgrp\": { \"value\": \"resgrp\" },\n        \"para_acronym_appsvc\": { \"value\": \"appsvc\" },\n        \"para_acronym_svcpln\": { \"value\": \"svcpln\" },\n        \"para_acronym_stract\": { \"value\": \"str\" },\n        \"para_acronym_kv\": { \"value\": \"kv\" },\n        \"para_acronym_azfunc\": { \"value\": \"fn\" },\n        \"para_acronym_appin\": { \"value\": \"appins\" },\n        \"para_acronym_webapp\": { \"value\": \"webapp\" },\n        \"para_ad_tenantid\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_application_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_vanity_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_target_env\": { \"value\": \"dev\" },\n        \"para_kvSecretsObject\": {\n            \"value\": {\n                \"secrets\": [\n                {\n                    \"secretName\": \"applicationuser\",\n                    \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                },\n                {\n                    \"secretName\": \"AnotherSecrect\",\n                    \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                }\n                ]\n            }\n        }\n    }\n}\n\nVariables\nAs you can see from the variables, I build up my resource names from the parameters.\nI also pull in values for the hostingplan and component identities, so they can be used easily with the resource definitions.\n\"variables\": {\n    \"var_env_region\": \"[concat(parameters('para_target_env'), '-', parameters('para_acronym_region'))]\",\n    \"var_public_url\": \"[concat(parameters('para_target_env'), '.', parameters('para_application_name'), '.', parameters('para_vanity_name'))]\",\n    \"var_str_name\": \"[concat(parameters('para_application_name'), parameters('para_acronym_stract'), parameters('para_target_env'), parameters('para_acronym_region'))]\",\n    \"var_str_resId\": \"[resourceId(resourceGroup().Name,'Microsoft.Storage/storageAccounts', variables('var_str_name'))]\",\n    \"var_kv_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_kv'), '-', variables('var_env_region'))]\",\n    \"var_azf_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_azfunc'),'-', variables('var_env_region'))]\",\n    \"var_appin_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appin'),'-', variables('var_env_region'))]\",\n    \"var_hstpln_group\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_resgrp'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_env\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appsvc'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_svcpln'), '-', variables('var_env_region'))]\",\n    \"var_webapp_name\": \"[concat(parameters('para_application_name'), '-' ,parameters('para_acronym_webapp'),'-', variables('var_env_region'))]\",\n    \"var_webapp_hstpln\": \"[concat('/subscriptions/', subscription().subscriptionId, '/resourceGroups/', variables('var_hstpln_group'), '/providers/Microsoft.Web/serverfarms/', variables('var_hstpln_name'))]\",\n    \"var_msi_azf\": \"[concat(resourceId('Microsoft.Web/sites', variables('var_azf_name')),'/providers/Microsoft.ManagedIdentity/Identities/default')]\"\n},"
  },
  {
    "objectID": "Journal/2018-10-15-AzureFunctionManagedServiceIdentities.html",
    "href": "Journal/2018-10-15-AzureFunctionManagedServiceIdentities.html",
    "title": "Azure Function Managed Service Identities",
    "section": "",
    "text": "Bootstrapping\nThe trouble with many security policies is that at least some element needs to know the password in order to instigate access to resources. That used to mean putting credentials into a configuration file or inserting them during a deployment process. The Manage Service Identities (MSI) facility has got around this by allowing all your resources to register a service principal with Active Directory, and then each resource grants the desired level of access to that service principal. By doing the security in this way, each of the resources never need to know credentials, they only request access and deal with the response. So, by removing credentials from the equation then there is no need to have to rotate passwords or update certs on a timely basis as they simply don�t exist between the resources.\n\n\nSo how do we accomplish this.\nWithin the azure function arm template declaration insert the following, this will register the function with your active directory.\n\"identity\": {\"type\": \"SystemAssigned\"},\nIn the variables section of the Arm Template, get the identity of the Azure Function. (replace ‘var_azf_name’ with the name of your function)\n\"var_msi_azf\": \"[concat(resourceId('Microsoft.Web/sites', variables('var_azf_name')),'/providers/Microsoft.ManagedIdentity/Identities/default')]\"\nWithin your Key Vault template your will need to add the functions access policy\n\"accessPolicies\": [{\"tenantId\": \"[reference(variables('var_msi_azf'), '2015-08-31-PREVIEW').tenantId]\",\"objectId\": \"[reference(variables('var_msi_azf'), '2015-08-31-PREVIEW').principalId]\",\"permissions\": {\"certificates\": [\"get\"],\"keys\": [\"get\"],\"secrets\": [\"get\"]}}}]"
  },
  {
    "objectID": "Journal/2019-02-11-Azure-C4.html",
    "href": "Journal/2019-02-11-Azure-C4.html",
    "title": "Azure/C4",
    "section": "",
    "text": "I found a good resource, so I can include bot C4 diagrams and Azure icons within a PlantUML diagram. I’ve done C4 within PlantUml before but it was a bit crude.\n![C4 flowchart](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.github.com/newportg/newportg.github.io/master/assets/C4/c4.puml)\n\n\n\nC4 flowchart\n\n\n![Azure flowchart](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.github.com/newportg/newportg.github.io/master/assets/C4/azure.puml)\n\n\n\nAzure flowchart\n\n\nhttps://github.com/RicardoNiepel/Azure-PlantUML"
  },
  {
    "objectID": "Journal/2019-02-11-WOPI.html",
    "href": "Journal/2019-02-11-WOPI.html",
    "title": "WOPI",
    "section": "",
    "text": "For a full description of WOPI, please follow the link to the project. WOPI Project\n\n\n\nWopi Balanced"
  },
  {
    "objectID": "Journal/2019-02-15-StorageAccounts.html",
    "href": "Journal/2019-02-15-StorageAccounts.html",
    "title": "Storage Accounts",
    "section": "",
    "text": "The blob container resource is a sub-resource of the storage account. When you create a storage account, you can now specify an array of storages resources. We are then going to specify objects of type “blobServices/containers”, and make sure to use an API version of 2018-02-01 or later. In the example below, we are creating a storage account with two containers.\n{\n    \"type\": \"Microsoft.Storage/storageAccounts\",\n    \"apiVersion\": \"2018-02-01\",\n    \"name\": \"[parameters('StorageAccountName')]\",\n    \"location\": \"[resourceGroup().location]\",\n    \"tags\": {\n        \"displayName\": \"[parameters('StorageAccountName')]\"\n    },\n    \"sku\": {\n        \"name\": \"Standard_LRS\"\n    },\n    \"kind\": \"StorageV2\",\n    \"properties\": {},\n    \"resources\": [\n        {\n            \"type\": \"blobServices/containers\",\n            \"apiVersion\": \"2018-03-01-preview\",\n            \"name\": \"[concat('default/', parameters('Container1Name'))]\",\n            \"dependsOn\": [\n                \"[parameters('StorageAccountName')]\"\n            ],\n            \"properties\": {\n                \"publicAccess\": \"Container\"\n            }\n        },\n        {\n            \"type\": \"blobServices/containers\",\n            \"apiVersion\": \"2018-03-01-preview\",\n            \"name\": \"[concat('default/', parameters('Container2Name'))]\",\n            \"dependsOn\": [\n                \"[parameters('StorageAccountName')]\"\n            ],\n            \"properties\": {\n                \"publicAccess\": \"None\"\n            }\n        }\n    ]\n}\nWe can then go ahead and deploy this, and see the two containers being created. You can see in the template we are setting different Public Access properties on the two containers, you have a choice of 3 values here: None (private container) Container (the whole container is publically accessible) Blob (only Blobs are publically accessible)\nThe above example is fine, but you need to specify each of the containers as separate parameters, and remember to add the correct number of container clauses to the script.\nSo if we specify the containers as a array in the parameters file\n    \"para_storageObject\": {\n      \"value\": {\n        \"containers\": [\n          {\n            \"containerName\": \"files\"\n          },\n          {\n            \"containerName\": \"elephants\"\n          },\n          {\n            \"containerName\": \"bananas\"\n          }\n        ]\n      }\n    },\nThen we can use a copy clause to cycle over the array and create our containers in the deploy\n    {\n      \"apiVersion\": \"2018-03-01-preview\",\n      \"copy\": {\n        \"name\": \"containersCopy\",\n        \"count\": \"[length(parameters('para_storageObject').containers)]\"\n      },\n      \"dependsOn\": [\n        \"[concat('Microsoft.Storage/storageAccounts/', variables('var_str_name'))]\"\n      ],\n      \"name\": \"[concat(variables('var_str_name'), '/default/', parameters('para_storageObject').containers[copyIndex()].containerName)]\",\n      \"properties\": {\n        \"publicAccess\": \"Container\"\n      },\n      \"tags\": {\n        \"displayName\": \"StorageAcct/Containers\"\n      },\n      \"type\": \"Microsoft.Storage/storageAccounts/blobServices/containers\"\n    },"
  },
  {
    "objectID": "Journal/2018/10/2018-10-15-AzureFunctionManagedServiceIdentities.html",
    "href": "Journal/2018/10/2018-10-15-AzureFunctionManagedServiceIdentities.html",
    "title": "Azure Function Managed Service Identities",
    "section": "",
    "text": "Bootstrapping\nThe trouble with many security policies is that at least some element needs to know the password in order to instigate access to resources. That used to mean putting credentials into a configuration file or inserting them during a deployment process. The Manage Service Identities (MSI) facility has got around this by allowing all your resources to register a service principal with Active Directory, and then each resource grants the desired level of access to that service principal. By doing the security in this way, each of the resources never need to know credentials, they only request access and deal with the response. So, by removing credentials from the equation then there is no need to have to rotate passwords or update certs on a timely basis as they simply don�t exist between the resources.\n\n\nSo how do we accomplish this.\nWithin the azure function arm template declaration insert the following, this will register the function with your active directory.\n\"identity\": {\"type\": \"SystemAssigned\"},\nIn the variables section of the Arm Template, get the identity of the Azure Function. (replace ‘var_azf_name’ with the name of your function)\n\"var_msi_azf\": \"[concat(resourceId('Microsoft.Web/sites', variables('var_azf_name')),'/providers/Microsoft.ManagedIdentity/Identities/default')]\"\nWithin your Key Vault template your will need to add the functions access policy\n\"accessPolicies\": [{\"tenantId\": \"[reference(variables('var_msi_azf'), '2015-08-31-PREVIEW').tenantId]\",\"objectId\": \"[reference(variables('var_msi_azf'), '2015-08-31-PREVIEW').principalId]\",\"permissions\": {\"certificates\": [\"get\"],\"keys\": [\"get\"],\"secrets\": [\"get\"]}}}]"
  },
  {
    "objectID": "Journal/2018/10/2018-10-10-Arm-Template-ParameterVariable-Setup.html",
    "href": "Journal/2018/10/2018-10-10-Arm-Template-ParameterVariable-Setup.html",
    "title": "ARM TEMPLATE PARAMETER/VARIABLE SETUP",
    "section": "",
    "text": "For something so simple, arm templates can become complex things, so I prefer to try to set some ground rules before I go to deep.\nN.B this works for me, and may not suit everyone 😉\nYou should employee a naming convention for your artifacts. Every Resource should be tagged. There should be a clear naming convention between the parameters and variables. Parameters should be either primitives or unique values Variables should build up your resource names from the parameter primitives. ### Parameters I prefer to inject any unique values via a VSTS/VSO or if your prefer Azure DevOps deployment process.\nIn the first part of the file I spell out the acronyms which form part of the naming convention for the resources, you could use nested templates for this, but I feel the add unnecessary complications, as the nested template must be available via a URL.\nThe second part involves parameters that are specific to this application, such as the tenant id, application name etc.\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"para_acronym_region\": { \"value\": \"we\" },\n        \"para_acronym_resgrp\": { \"value\": \"resgrp\" },\n        \"para_acronym_appsvc\": { \"value\": \"appsvc\" },\n        \"para_acronym_svcpln\": { \"value\": \"svcpln\" },\n        \"para_acronym_stract\": { \"value\": \"str\" },\n        \"para_acronym_kv\": { \"value\": \"kv\" },\n        \"para_acronym_azfunc\": { \"value\": \"fn\" },\n        \"para_acronym_appin\": { \"value\": \"appins\" },\n        \"para_acronym_webapp\": { \"value\": \"webapp\" },\n        \"para_ad_tenantid\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_application_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_vanity_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_target_env\": { \"value\": \"dev\" },\n        \"para_kvSecretsObject\": {\n            \"value\": {\n                \"secrets\": [\n                {\n                    \"secretName\": \"applicationuser\",\n                    \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                },\n                {\n                    \"secretName\": \"AnotherSecrect\",\n                    \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                }\n                ]\n            }\n        }\n    }\n}\n\nVariables\nAs you can see from the variables, I build up my resource names from the parameters.\nI also pull in values for the hostingplan and component identities, so they can be used easily with the resource definitions.\n\"variables\": {\n    \"var_env_region\": \"[concat(parameters('para_target_env'), '-', parameters('para_acronym_region'))]\",\n    \"var_public_url\": \"[concat(parameters('para_target_env'), '.', parameters('para_application_name'), '.', parameters('para_vanity_name'))]\",\n    \"var_str_name\": \"[concat(parameters('para_application_name'), parameters('para_acronym_stract'), parameters('para_target_env'), parameters('para_acronym_region'))]\",\n    \"var_str_resId\": \"[resourceId(resourceGroup().Name,'Microsoft.Storage/storageAccounts', variables('var_str_name'))]\",\n    \"var_kv_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_kv'), '-', variables('var_env_region'))]\",\n    \"var_azf_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_azfunc'),'-', variables('var_env_region'))]\",\n    \"var_appin_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appin'),'-', variables('var_env_region'))]\",\n    \"var_hstpln_group\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_resgrp'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_env\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appsvc'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_svcpln'), '-', variables('var_env_region'))]\",\n    \"var_webapp_name\": \"[concat(parameters('para_application_name'), '-' ,parameters('para_acronym_webapp'),'-', variables('var_env_region'))]\",\n    \"var_webapp_hstpln\": \"[concat('/subscriptions/', subscription().subscriptionId, '/resourceGroups/', variables('var_hstpln_group'), '/providers/Microsoft.Web/serverfarms/', variables('var_hstpln_name'))]\",\n    \"var_msi_azf\": \"[concat(resourceId('Microsoft.Web/sites', variables('var_azf_name')),'/providers/Microsoft.ManagedIdentity/Identities/default')]\"\n},"
  },
  {
    "objectID": "Journal/2018/08/2018-08-14-AzureApplicationHosting.html",
    "href": "Journal/2018/08/2018-08-14-AzureApplicationHosting.html",
    "title": "Azure Application Hosting",
    "section": "",
    "text": "Each component is a separately deploy-able artefact, but we need a coherent single URL to link them all. The normal method would be to deploy out each individual component to Azure and each would get its own ‘aurewebsites.com’ URL. This approach would lead to confusion, as it would mean you would need to keep lists of URL’s By using the proxy feature of Azure Functions we can define routes to each of the installed artefacts while preserving a single URL for the application. So:-\n\n\n\nRoute\nResult\n\n\n\n\nzoomalong.co.uk\nWebsite\n\n\nzoomalong.co.uk/api\nAzure functions\n\n\nzoomalong.co.uk/static\nAzure Storage Account\n\n\n\nUsed to store any files or images etc. Because both the API and Web Application exist on the same URL then we won’t run into any CORS issues. Remember to bind the DNS Cname to the Azure function proxy and not the website.\n\n\n\naah1\n\n\nHow ? In your Azure Functions Project Create a files called proxies.json and insert the following code\nproxies.json\n{\n  \"$schema\": \"http://json.schemastore.org/proxies\",\n    \"proxies\": {\n      \"api\": {\n        \"matchCondition\": {\n          \"route\": \"/api/{*url}\"\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      },\n      \"app\": {\n        \"matchCondition\": {\n          \"route\": \"{*url}\",\n          \"methods\": [ \"GET\", \"HEAD\", \"OPTIONS\" ]\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      },\n      \"appResources\": {\n        \"matchCondition\": {\n          \"route\": \"/static/{*url}\",\n          \"methods\": [ \"GET\", \"HEAD\", \"OPTIONS\" ]\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      }\n    }\n  }\nChange in the build. * Build should include Azure App Service Deploy V3 or greater * Update this section with the file to be changed. *  * Add your substitutions to the Variables section * As Path to variable to be replace element.element.element * Value to be replaced. * as below\n\n\n\n\n\n\n\nProxy\nURL\n\n\n\n\nproxies.api.backendUri\nhttps://azure website url\n\n\nproxies.app.backendUri\nhttps://azure function url\n\n\nproxies.appResources.backendUri\nhttps:// azure storage account blob strorage\n\n\n\n\nRepeat in the release."
  },
  {
    "objectID": "Journal/2018/08/2018-08-14-Hosting AzureStaticWebsites.html",
    "href": "Journal/2018/08/2018-08-14-Hosting AzureStaticWebsites.html",
    "title": "Hosting Azure Static Websites",
    "section": "",
    "text": "Microsoft announced that you can now enable static websites on a storage account. This will generate a new URL for your site, and enable read access to any static html files within the blob storage. There’s a link to Microsoft’s preview announcement here. https://azure.microsoft.com/en-us/blog/azure-storage-static-web-hosting-public-preview/\nAt the moment I prefer the approach made by Anthony Chu in his blog post https://anthonychu.ca/post/azure-functions-static-file-server/\nHe hosts a index.html file within a Azure function Http Trigger request. Although this only returns a static file, it does allow you to create a on demand website. The Html file you serve up can should only contain links to CDN resources or to readable JS or other files within your storage account blob storage. By proxying the azure function then everything could be accessed via the same URL.\nusing System.IO;\nusing System.Linq;\nusing System.Net;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Threading.Tasks;\nusing Microsoft.Azure.WebJobs;\nusing Microsoft.Azure.WebJobs.Extensions.Http;\nusing Microsoft.Azure.WebJobs.Host;\nusing MimeTypes;\n\nnamespace Counterflip\n{\n    public static class WebSite\n    {\n        [FunctionName(\"WebSite\")]\n        public static async Task&lt;HttpResponseMessage&gt; Run([HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)]HttpRequestMessage req, TraceWriter log)\n        {\n            log.Info(\"C# HTTP trigger function processed a request.\");\n\n            try\n            {\n                var response = new HttpResponseMessage(HttpStatusCode.OK);\n                var stream = new FileStream(@\"www\\index.html\", FileMode.Open);\n                response.Content = new StreamContent(stream);\n                response.Content.Headers.ContentType =\n                    new MediaTypeHeaderValue(GetMimeType(@\"www\\index.html\"));\n                return response;\n            }\n            catch\n            {\n                return new HttpResponseMessage(HttpStatusCode.NotFound);\n            }\n        }\n\n        private static string GetMimeType(string filePath)\n        {\n            var fileInfo = new FileInfo(filePath);\n            return MimeTypeMap.GetMimeType(fileInfo.Extension);\n        }\n    }\n}"
  },
  {
    "objectID": "Journal/2018/10/2018-10-15-ARMTemplateParameter-VariableSetup.html",
    "href": "Journal/2018/10/2018-10-15-ARMTemplateParameter-VariableSetup.html",
    "title": "ARM Template Parameter/Variable Setup",
    "section": "",
    "text": "For something so simple, arm templates can become complex things, so I prefer to try to set some ground rules before I go to deep. N.B this works for me, and may not suit everyone 😉"
  },
  {
    "objectID": "Journal/2018/10/2018-10-15-ARMTemplateParameter-VariableSetup.html#parameters",
    "href": "Journal/2018/10/2018-10-15-ARMTemplateParameter-VariableSetup.html#parameters",
    "title": "ARM Template Parameter/Variable Setup",
    "section": "Parameters",
    "text": "Parameters\nI prefer to inject any unique values via a VSTS/VSO or if your prefer Azure DevOps deployment process. In the first part of the file I spell out the acronyms which form part of the naming convention for the resources, you could use nested templates for this, but I feel they add unnecessary complications, as the nested template must be available via a URL. The second part involves parameters that are specific to this application, such as the tenant id, application name etc.\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"para_acronym_region\": { \"value\": \"we\" },\n        \"para_acronym_resgrp\": { \"value\": \"resgrp\" },\n        \"para_acronym_appsvc\": { \"value\": \"appsvc\" },\n        \"para_acronym_svcpln\": { \"value\": \"svcpln\" },\n        \"para_acronym_stract\": { \"value\": \"str\" },\n        \"para_acronym_kv\": { \"value\": \"kv\" },\n        \"para_acronym_azfunc\": { \"value\": \"fn\" },\n        \"para_acronym_appin\": { \"value\": \"appins\" },\n        \"para_acronym_webapp\": { \"value\": \"webapp\" },\n        \"para_ad_tenantid\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_application_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_vanity_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_target_env\": { \"value\": \"dev\" },\n        \"para_kvSecretsObject\": {\n            \"value\": {\n                \"secrets\": [\n                        {\n                        \"secretName\": \"applicationuser\",\n                        \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                        },\n                        {\n                            \"secretName\": \"AnotherSecrect\",\n                            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                        }\n                    ]\n                }\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "Journal/2018/10/2018-10-15-ARMTemplateParameter-VariableSetup.html#variables",
    "href": "Journal/2018/10/2018-10-15-ARMTemplateParameter-VariableSetup.html#variables",
    "title": "ARM Template Parameter/Variable Setup",
    "section": "Variables",
    "text": "Variables\nAs you can see from the variables, I build up my resource names from the parameters. I also pull in values for the hostingplan and component identities, so they can be used easily with the resource definitions.\n\"variables\": {\n    \"var_env_region\": \"[concat(parameters('para_target_env'), '-', parameters('para_acronym_region'))]\",\n    \"var_public_url\": \"[concat(parameters('para_target_env'), '.', parameters('para_application_name'), '.', parameters('para_vanity_name'))]\",\n    \"var_str_name\": \"[concat(parameters('para_application_name'), parameters('para_acronym_stract'), parameters('para_target_env'), parameters('para_acronym_region'))]\",\n    \"var_str_resId\": \"[resourceId(resourceGroup().Name,'Microsoft.Storage/storageAccounts', variables('var_str_name'))]\",\n    \"var_kv_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_kv'), '-', variables('var_env_region'))]\",\n    \"var_azf_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_azfunc'),'-', variables('var_env_region'))]\",\n    \"var_appin_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appin'),'-', variables('var_env_region'))]\",\n    \"var_hstpln_group\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_resgrp'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_env\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appsvc'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_svcpln'), '-', variables('var_env_region'))]\",\n    \"var_webapp_name\": \"[concat(parameters('para_application_name'), '-' ,parameters('para_acronym_webapp'),'-', variables('var_env_region'))]\",\n    \"var_webapp_hstpln\": \"[concat('/subscriptions/', subscription().subscriptionId, '/resourceGroups/', variables('var_hstpln_group'), '/providers/Microsoft.Web/serverfarms/', variables('var_hstpln_name'))]\",\n    \"var_msi_azf\": \"[concat(resourceId('Microsoft.Web/sites', variables('var_azf_name')),'/providers/Microsoft.ManagedIdentity/Identities/default')]\"\n},"
  },
  {
    "objectID": "Journal/index.html",
    "href": "Journal/index.html",
    "title": "Blog",
    "section": "",
    "text": "March 4, 2019\n        \n        \n            Storage Account Encryption\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Storage Accounts\n                \n                \n            \n            \n\n            How to use the Glyphs panel in InDesign CS3 to insert Arabic text, despite the lack of\n            \n        \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "Journal/index.html#posts",
    "href": "Journal/index.html#posts",
    "title": "Blog",
    "section": "",
    "text": "March 4, 2019\n        \n        \n            Storage Account Encryption\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Storage Accounts\n                \n                \n            \n            \n\n            How to use the Glyphs panel in InDesign CS3 to insert Arabic text, despite the lack of\n            \n        \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "Journal/2025-10-04-blogpost.html",
    "href": "Journal/2025-10-04-blogpost.html",
    "title": "Storage Account Encryption",
    "section": "",
    "text": "By default storage accounts are encrypted, and Microsoft holds the keys. The encryption that is used is AES 256 bit, as it is one of the strongest ciphers currently available.\nThe one big issue with this is that Microsoft owns the encryption key and potentially has unrestricted access to the users data. They have included the facility for the user to specify an encryption key, so you can meet your individual company security or regulatory compliance needs. To use your own key, you will need. * A Key Vault * Needs to be in the same region as the storage * Does not need to be in the same subscription * Storage needs permissions to access your Key Vault. * Need to grant wrapKey, unwrapKey privileges"
  },
  {
    "objectID": "Journal/2025-10-04-blogpost.html#data-encryption",
    "href": "Journal/2025-10-04-blogpost.html#data-encryption",
    "title": "Storage Account Encryption",
    "section": "Data Encryption",
    "text": "Data Encryption\nA key point to understand, the data itself is not encrypted with the key. Microsoft employs a two stage encryption process which involves a DEK (Data Encryption Key) and a KEK (Key Encryption Key). The DEK is generated when the storage account is created, and is used to encrypt the data. The DEK is it self is encrypted with a key that Microsoft holds (KEK), Its this Microsoft key that can be replaced with the users own key."
  },
  {
    "objectID": "Journal/2025-10-04-blogpost.html#key-rotation",
    "href": "Journal/2025-10-04-blogpost.html#key-rotation",
    "title": "Storage Account Encryption",
    "section": "Key Rotation",
    "text": "Key Rotation\nThis is currently under development by Microsoft. Key rotation is a process where the KEK (see Data Encryption above) is rotated every 90 days, this involves decrypting the existing key and re-encrypting it with a newly generated key."
  },
  {
    "objectID": "Journal/2025-10-04-blogpost.html#references",
    "href": "Journal/2025-10-04-blogpost.html#references",
    "title": "Storage Account Encryption",
    "section": "References",
    "text": "References\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-service-encryption"
  }
]